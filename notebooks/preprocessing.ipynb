{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b503a84",
   "metadata": {},
   "source": [
    "### Preprocessing Goals\n",
    "\n",
    "- this notebook builds a robust text preprocessing pipeline for sentiment classification.\n",
    "The output is a clean text column `normalized_review` optimized for TF-IDF and linear models (in this project: Logistic Regression).\n",
    "\n",
    "- Input column: `review_body`\n",
    "- Output column: `normalized_review`\n",
    "\n",
    "- The pipeline keeps sentiment-relevant signals (for ex: negation and strong punctuation) and removes noisy artifacts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0333a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import contractions\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from typing import Iterable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e3d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a7286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nlp_ner():\n",
    "    try:\n",
    "        return spacy.load(\"en_core_web_md\", disable=[\"parser\", \"textcat\"])\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6613ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = load_nlp_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82802a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/raw/amazon_reviews_us_Digital_Software_v1_00.tsv\"\n",
    "\n",
    "NROWS = 10000\n",
    "loaded_data = pd.read_csv(file_path, sep=\"\\t\", encoding=\"utf-8\", nrows=NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5bdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loaded_data[\"review_body\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8216b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb34049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5451d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960e351",
   "metadata": {},
   "source": [
    "#### Regular Expression Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafa87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_REGEX_URL = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)  # case sensitive=False\n",
    "_REGEX_EMAIL = re.compile(r\"\\b[\\w.\\-+]+@[\\w.\\-]+\\.\\w+\\b\")\n",
    "_REGEX_USER = re.compile(r\"@[A-Za-z0-9_]+\")\n",
    "_REGEX_HASHTAG = re.compile(r\"#([A-Za-z0-9_]+)\")\n",
    "_REGEX_HTML_TAG = re.compile(r\"<[^>]+>\")\n",
    "_REGEX_CONTROL = re.compile(r\"[\\r\\n\\t]+\")\n",
    "_REGEX_MULTI_WS = re.compile(r\"\\s+\")\n",
    "_REGEX_REPEAT_CHAR = re.compile(r\"([A-Za-z])\\1{2,}\")\n",
    "_REGEX_DOTS = re.compile(r\"\\.{3,}\")\n",
    "_RE_INT = re.compile(r\"^\\d+$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91540a",
   "metadata": {},
   "source": [
    "#### Special Tokens\n",
    "\n",
    "These tokens are intentionally preserved because they carry sentiment information.\n",
    "\n",
    "The pipeline uses structured marker tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1714c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SPECIAL_TOKENS = {\n",
    "    \"tok_url\", \"tok_email\", \"tok_user\", \"tok_hashtag\",\n",
    "    \"tok_num\", \"tok_num_1\", \"tok_num_2\", \"tok_num_3\", \"tok_num_4\", \"tok_num_5\",\n",
    "    \"tok_excl\", \"tok_excl_multi\",\n",
    "    \"tok_q\", \"tok_q_multi\",\n",
    "    \"tok_qex\",\n",
    "    \"tok_org\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a57c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NEGATIONS = {\n",
    "    \"not\", \"no\", \"never\", \"none\", \"cannot\", \"can't\", \"dont\", \"nothing\", \"neither\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f9ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_EXCEPTIONS = {\n",
    "    \"i\", \"you\", \"we\", \"it\", \"they\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd4ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SCOPE_BREAKERS = {\"tok_excl\", \"tok_excl_multi\", \"tok_q\", \"tok_q_multi\", \"tok_qex\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4343835",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPE_BREAKERS = {\n",
    "    \"tok_excl\", \"tok_excl_multi\",\n",
    "    \"tok_q\", \"tok_q_multi\",\n",
    "    \"tok_qex\",\n",
    "    \"but\", \"however\", \"though\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b8a8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a01d91",
   "metadata": {},
   "source": [
    "#### NER\n",
    "\n",
    "company names are replaced with `tok_org`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f4f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_org_entities_batch(texts, nlp_ner, batch_size=1024):\n",
    "    if nlp_ner is None:\n",
    "        return list(texts)\n",
    "\n",
    "    out = []\n",
    "    docs = nlp_ner.pipe(texts, batch_size=batch_size)\n",
    "\n",
    "    for text, doc in zip(texts, docs):\n",
    "        if not doc.ents:\n",
    "            out.append(text)\n",
    "            continue\n",
    "\n",
    "        parts, last = [], 0\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ != \"ORG\":\n",
    "                continue\n",
    "            parts.append(text[last:ent.start_char])\n",
    "            parts.append(\" tok_org \")\n",
    "            last = ent.end_char\n",
    "        parts.append(text[last:])\n",
    "        out.append(\"\".join(parts))\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fad09",
   "metadata": {},
   "source": [
    "#### Convert contractions\n",
    "\n",
    "expands common English contractions into their full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daac1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f2169",
   "metadata": {},
   "source": [
    "#### Text normalization\n",
    "\n",
    "URL, email, mentions, hashtags, and punctuation markers are normalized to `tok_*` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    if text is None:\n",
    "        return ''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = expand_contractions(text)\n",
    "\n",
    "    text = _REGEX_HTML_TAG.sub(\" \", text)\n",
    "    text = _REGEX_URL.sub(\" tok_url \", text)\n",
    "    text = _REGEX_EMAIL.sub(\" tok_email \", text)\n",
    "    text = _REGEX_USER.sub(\" tok_user \", text)\n",
    "    text = _REGEX_HASHTAG.sub(r\" tok_hashtag \\1 \", text)\n",
    "\n",
    "    text = _REGEX_CONTROL.sub(\" \", text)\n",
    "    text = _REGEX_DOTS.sub(\" \", text)\n",
    "    text = _REGEX_REPEAT_CHAR.sub(r\"\\1\\1\", text)\n",
    "    text = _REGEX_CONTROL.sub(\" \", text)\n",
    "\n",
    "    # keep sentiment punctuation as explicit markers before dropping punctuation tokens\n",
    "    text = re.sub(r\"\\?\\!|\\!\\?\", \" tok_qex \", text)\n",
    "    text = re.sub(r\"\\!{2,}\", \" tok_excl_multi \", text)\n",
    "    text = re.sub(r\"\\?{2,}\", \" tok_q_multi \", text)\n",
    "    text = re.sub(r\"\\!\", \" tok_excl \", text)\n",
    "    text = re.sub(r\"\\?\", \" tok_q \", text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = _REGEX_MULTI_WS.sub(\" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e4caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalized_reviews = data.astype(str).to_list()\n",
    "loaded_data[\"normalized_review\"] = [normalize_text(x) for x in normalized_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981af771",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f851a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682697d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].iloc[10:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b021aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44afe6e",
   "metadata": {},
   "source": [
    "#### Map sentiment numbers\n",
    "\n",
    "maps numbers in the token that are relevant to sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0f84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment_number(token) -> str:\n",
    "    raw = token.text.strip()\n",
    "    if _RE_INT.match(raw):\n",
    "        value = int(raw)\n",
    "        if 1 <= value <= 5:\n",
    "            return f\"tok_num_{value}\"\n",
    "        return \"tok_num\"\n",
    "    if token.like_num:\n",
    "        return \"tok_num\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b931b",
   "metadata": {},
   "source": [
    "#### Token normalization\n",
    "\n",
    "lemmatization + numeric mapping (`tok_num`, `tok_num_1-5`) + optional stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "334a76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_token(token, use_lemma: bool = True, remove_stopwords: bool = False) -> str:\n",
    "    raw = token.text.strip()\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "\n",
    "    if raw in _SPECIAL_TOKENS:\n",
    "        return raw\n",
    "    \n",
    "    num_token = map_sentiment_number(token)\n",
    "    if num_token:\n",
    "        return num_token\n",
    "    \n",
    "    if token.is_punct or token.is_space:\n",
    "        return \"\"\n",
    "\n",
    "    normalized = token.lemma_.lower() if (use_lemma and token.lemma_ and token.lemma_ != \"-PRON-\") else token.lower_\n",
    "    normalized = normalized.strip(\"`'\\\".,;:()[]{}<>|\\\\/\")\n",
    "\n",
    "    if not normalized:\n",
    "        return \"\"\n",
    "    if len(normalized) < 2 and normalized not in _EXCEPTIONS:\n",
    "        return \"\"\n",
    "    if remove_stopwords and normalized in STOP_WORDS and normalized not in _NEGATIONS:\n",
    "        return \"\"\n",
    "\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4450477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = nlp.pipe(\n",
    "    loaded_data[\"normalized_review\"].fillna(\"\").astype(str).tolist(),\n",
    "    batch_size=1024\n",
    ")\n",
    "\n",
    "normalized_reviews = [\n",
    "    \" \".join(tok for tok in (normalize_token(t) for t in doc) if tok)\n",
    "    for doc in docs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loaded_data[\"normalized_review\"] = normalized_reviews\n",
    "loaded_data[[\"review_body\", \"normalized_review\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].sample(20)  # random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e091f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = nlp.pipe(\n",
    "    loaded_data[\"normalized_review\"].fillna(\"\").astype(str).tolist(),\n",
    "    batch_size=1024\n",
    ")\n",
    "\n",
    "normalized_reviews = [\n",
    "    \" \".join(\n",
    "        tok for tok in (normalize_token(t) for t in doc) if tok\n",
    "    )\n",
    "    for doc in docs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[\"normalized_review\"] = normalized_reviews\n",
    "loaded_data[[\"review_body\", \"normalized_review\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93d62d",
   "metadata": {},
   "source": [
    "#### Negation scope\n",
    "\n",
    "handling: tokens following negation words are augmented with `neg_` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e27518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_negation_scope(tokens: list[str], window: int = 3) -> list[str]:\n",
    "    out: list[str] = []\n",
    "    n = len(tokens)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        tok = tokens[i]\n",
    "        out.append(tok)\n",
    "\n",
    "        if tok in _NEGATIONS:\n",
    "            applied = 0\n",
    "            j = i + 1\n",
    "            while j < n and applied < window:\n",
    "                nxt = tokens[j]\n",
    "                if nxt in _SCOPE_BREAKERS:\n",
    "                    break\n",
    "                if nxt and not nxt.startswith(\"tok_\") and nxt not in _NEGATIONS:\n",
    "                    out.append(f\"neg_{nxt}\")\n",
    "                    applied += 1\n",
    "                j += 1\n",
    "        i += 1\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1faab8",
   "metadata": {},
   "source": [
    "#### Core Preprocessing Functions\n",
    "\n",
    "**`preprocess_one`**\n",
    "\n",
    "Preprocesses a single text sample end-to-end.\n",
    "It applies text normalization, token normalization, and negation-scope expansion.\n",
    "Useful for debugging, unit checks, and single-input inference tests.\n",
    "\n",
    "**`preprocess_series`**\n",
    "\n",
    "Preprocesses a collection of texts in batch mode.\n",
    "It runs ORG masking with `nlp_ner.pipe`, then normalization and token processing with `nlp.pipe`.\n",
    "Designed for efficient large-scale processing and consistent output formatting.\n",
    "\n",
    "**`preprocess_dataframe`**\n",
    "\n",
    "Preprocesses a full DataFrame and writes the cleaned output to a target column.\n",
    "It validates input columns, runs the batched preprocessing pipeline, and returns a copied DataFrame.\n",
    "This is the main entry point used in this notebook for final dataset generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "133f0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one(text: str, nlp, use_lemma: bool = True, remove_stopwords: bool = False, negation_window: int = 3) -> str:\n",
    "    clean = normalize_text(text)\n",
    "    if not clean:\n",
    "        return \"\"\n",
    "    doc = nlp(clean)\n",
    "    tokens = [normalize_token(tok, use_lemma=use_lemma, remove_stopwords=remove_stopwords) for tok in doc]\n",
    "    tokens = [t for t in tokens if t]\n",
    "    tokens = apply_negation_scope(tokens, window=negation_window)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_series(\n",
    "    texts: Iterable[str],\n",
    "    nlp,\n",
    "    nlp_ner=None,\n",
    "    batch_size: int = 1024,\n",
    "    use_lemma: bool = True,\n",
    "    remove_stopwords: bool = False,\n",
    "    negation_window: int = 3,\n",
    ") -> list[str]:\n",
    "    \n",
    "    raw_texts = [str(t) for t in texts]\n",
    "\n",
    "    # 1) NER masking (org)\n",
    "    masked_texts = mask_org_entities_batch(raw_texts, nlp_ner, batch_size=batch_size)\n",
    "\n",
    "    # 2) regex normalization\n",
    "    normalized_texts = (normalize_text(t) for t in masked_texts)\n",
    "\n",
    "    # 3) token pipeline\n",
    "    docs = nlp.pipe(normalized_texts, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    out: list[str] = []\n",
    "    for doc in docs:\n",
    "        tokens = [normalize_token(tok, use_lemma=use_lemma, remove_stopwords=remove_stopwords) for tok in doc]\n",
    "        tokens = [t for t in tokens if t]\n",
    "        tokens = apply_negation_scope(tokens, window=negation_window)\n",
    "        out.append(\" \".join(tokens))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f425fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"review_body\",\n",
    "    output_col: str = \"normalized_review\",\n",
    "    use_lemma: bool = True,\n",
    "    remove_stopwords: bool = False,\n",
    "    negation_window: int = 3,\n",
    "    batch_size: int = 1024,\n",
    ") -> pd.DataFrame:\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_col}' not found.\")\n",
    "\n",
    "    processed = preprocess_series(\n",
    "        df[text_col].fillna(\"\").astype(str).values,\n",
    "        nlp=nlp,\n",
    "        nlp_ner=nlp_ner,\n",
    "        batch_size=batch_size,\n",
    "        use_lemma=use_lemma,\n",
    "        remove_stopwords=remove_stopwords,\n",
    "        negation_window=negation_window,\n",
    "    )\n",
    "    out_df = df.copy()\n",
    "    out_df[output_col] = processed\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184af1ad",
   "metadata": {},
   "source": [
    "processed text is saved to normalized_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = preprocess_dataframe(\n",
    "    df=loaded_data,\n",
    "    text_col=\"review_body\",\n",
    "    output_col=\"normalized_review\",\n",
    "    use_lemma=True,\n",
    "    remove_stopwords=True,\n",
    "    negation_window=3,\n",
    "    batch_size=1024\n",
    ")\n",
    "\n",
    "loaded_data = out_df\n",
    "loaded_data[[\"review_body\", \"normalized_review\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00881ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[[\"review_body\", \"normalized_review\"]].sample(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
